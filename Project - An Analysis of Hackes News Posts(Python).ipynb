{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Analysis of News Posts on a Popular Website (Hacker News)\n",
    "This project analyses user content on a popular technology website \"Hacker News\". Dataset containing user information has been downloaded from [kaggle](https://www.kaggle.com/hacker-news/hacker-news-posts/kernels). Two types of posts \"Ask HN\" and \"Show HN\" are more thoroughly regarding contents, ratings, timings etc.\n",
    "\n",
    "### Opening and Preliminary Exploration of Data\n",
    "dataset is opened in code editor using `open` command, and then read using `import` and `reader`commands. Header, i.e. column heading is assigned to separate variable and then the first row containing title is removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(dataset,start,end,rows_and_columns=False):\n",
    "        dataset_slice=dataset[start:end]\n",
    "        for row in dataset_slice:\n",
    "            print(row)\n",
    "            print('\\n') #adds an empty line after each row\n",
    "        if rows_and_columns:\n",
    "            print('Number of rows',len(dataset))\n",
    "            print('Number of Columns',len(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn full Columns: ['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'] \n",
      "\n",
      "['12579008', 'You have two days to comment if you want stem cells to be classified as your own', 'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018', '1', '0', 'altstar', '9/26/2016 3:26']\n",
      "\n",
      "\n",
      "['12579005', 'SQLAR  the SQLite Archiver', 'https://www.sqlite.org/sqlar/doc/trunk/README.md', '1', '0', 'blacksqr', '9/26/2016 3:24']\n",
      "\n",
      "\n",
      "['12578997', 'What if we just printed a flatscreen television on the side of our boxes?', 'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43', '1', '0', 'pavel_lishin', '9/26/2016 3:19']\n",
      "\n",
      "\n",
      "['12578989', 'algorithmic music', 'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext', '1', '0', 'poindontcare', '9/26/2016 3:16']\n",
      "\n",
      "\n",
      "['12578979', 'How the Data Vault Enables the Next-Gen Data Warehouse and Data Lake', 'https://www.talend.com/blog/2016/05/12/talend-and-Â\\x93the-data-vaultÂ\\x94', '1', '0', 'markgainor1', '9/26/2016 3:14']\n",
      "\n",
      "\n",
      "Number of rows 293119\n",
      "Number of Columns 7\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "opened_file_hn=open('HN_posts.csv',encoding=\"utf8\")\n",
    "from csv import reader\n",
    "read_file_hn=reader(opened_file_hn)\n",
    "hn_full=list(read_file_hn)\n",
    "hn_header=hn_full[0]\n",
    "hn_full=hn_full[1:]\n",
    "print('hn full Columns:',hn_header,'\\n')\n",
    "hn_5rows= explore_dataset(hn_full,0,5,True)\n",
    "print(hn_5rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data-I\n",
    "- Removing entries with \"no comments\". This reduces \"293119\" entries to approx \"80000\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn Columns: ['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'] \n",
      "\n",
      "['12578975', 'Saving the Hassle of Shopping', 'https://blog.menswr.com/2016/09/07/whats-new-with-your-style-feed/', '1', '1', 'bdoux', '9/26/2016 3:13']\n",
      "\n",
      "\n",
      "['12578908', 'Ask HN: What TLD do you use for local development?', '', '4', '7', 'Sevrene', '9/26/2016 2:53']\n",
      "\n",
      "\n",
      "['12578822', 'Amazons Algorithms Dont Find You the Best Deals', 'https://www.technologyreview.com/s/602442/amazons-algorithms-dont-find-you-the-best-deals/', '1', '1', 'yarapavan', '9/26/2016 2:26']\n",
      "\n",
      "\n",
      "['12578694', 'Emergency dose of epinephrine that does not cost an arm and a leg', 'http://m.imgur.com/gallery/th6Ua', '2', '1', 'dredmorbius', '9/26/2016 1:54']\n",
      "\n",
      "\n",
      "['12578624', 'Phone Makers Could Cut Off Drivers. So Why Dont They?', 'http://www.nytimes.com/2016/09/25/technology/phone-makers-could-cut-off-drivers-so-why-dont-they.html', '4', '1', 'danso', '9/26/2016 1:37']\n",
      "\n",
      "\n",
      "Number of rows 80401\n",
      "Number of Columns 7\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hn_big=[]\n",
    "for row in hn_full:\n",
    "    num_comments=row[4]\n",
    "    if num_comments!='0':\n",
    "        hn_big.append(row)\n",
    "print('hn Columns:',hn_header,'\\n')\n",
    "hn_5rows= explore_dataset(hn_big,0,5,True)\n",
    "print(hn_5rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data-II\n",
    "- Using \"sample\" method to reduce entries to 20,000 through random sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn Columns: ['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'] \n",
      "\n",
      "['12354992', 'Ask HN: What should I know about FDM Group?', '', '1', '1', 'greyostrich', '8/24/2016 20:27']\n",
      "\n",
      "\n",
      "['11579098', 'Rotonde: the IOT library', 'https://github.com/HackerLoop/rotonde/tree/master', '10', '2', 'ges', '4/27/2016 10:11']\n",
      "\n",
      "\n",
      "['10692656', 'Story of Diaspora community manager', 'https://medium.com/anti-fiction/planting-a-seed-what-working-at-diaspora-was-like-cde26fa29364#.gyo9c9aoy', '3', '1', 'zlatan_todoric', '12/7/2015 21:00']\n",
      "\n",
      "\n",
      "['10990621', 'Banking Model of Education', 'https://en.wikipedia.org/wiki/Banking_education', '3', '1', 'Kinnard', '1/28/2016 19:39']\n",
      "\n",
      "\n",
      "['12091931', 'The Downside to Cord-Cutting', 'http://www.nytimes.com/2016/07/14/technology/personaltech/the-downside-to-cord-cutting.html?smid=fb-nytimes&smtyp=cur&_r=0', '2', '1', 'prostoalex', '7/14/2016 5:59']\n",
      "\n",
      "\n",
      "Number of rows 20000\n",
      "Number of Columns 7\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from random import sample\n",
    "hn=sample(hn_big,20000)\n",
    "print('hn Columns:',hn_header,'\\n')\n",
    "hn_5rows= explore_dataset(hn,0,5,True)\n",
    "print(hn_5rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data-III\n",
    "- Segregating posts with titles \"Ask HN\" and \"Show HN\" from \"Other Posts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ask_posts=[]\n",
    "show_posts=[]\n",
    "other_posts=[]\n",
    "for row in hn:\n",
    "    title=row[1]\n",
    "    title=title.lower()\n",
    "    \n",
    "    if title.startswith('ask hn'):\n",
    "        ask_posts.append(row)\n",
    "    elif title.startswith('show hn'):    \n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification\n",
    "In order to verify, we use the function ```explore_dataset()``` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask posts Columns: ['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'] \n",
      "\n",
      "['12354992', 'Ask HN: What should I know about FDM Group?', '', '1', '1', 'greyostrich', '8/24/2016 20:27']\n",
      "\n",
      "\n",
      "['11644667', 'Ask HN: Unknown certificate authority', '', '1', '2', 'andrewfromx', '5/6/2016 15:37']\n",
      "\n",
      "\n",
      "['10595325', 'Ask HN: How can I influence a change in this development process?', '', '5', '4', 'notinreallife', '11/19/2015 15:42']\n",
      "\n",
      "\n",
      "['11212623', 'Ask HN: Was Mark Zuckerberg wrong to call All Lives Matter employees malicious?', '', '5', '14', 'alllivesmatter', '3/2/2016 20:08']\n",
      "\n",
      "\n",
      "['12300433', \"Ask HN: What's the best way to do lighting in 2016?\", '', '2', '2', 'jMyles', '8/16/2016 20:33']\n",
      "\n",
      "\n",
      "Number of rows 1656\n",
      "Number of Columns 7\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('ask posts Columns:',hn_header,'\\n')\n",
    "ask_posts_5rows= explore_dataset(ask_posts,0,5,True)\n",
    "print(ask_posts_5rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show posts Columns: ['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'] \n",
      "\n",
      "['11432026', 'Show HN: In-Depth Guide to Choosing a Website Builder', 'http://www.sitebuilderreport.com/', '2', '5', 'steve-benjamins', '4/5/2016 16:50']\n",
      "\n",
      "\n",
      "['10219223', 'Show HN: Playing with the Google Charts API  Trending News', 'http://techwatching.com/discover.php', '4', '2', 'techwatching', '9/15/2015 6:18']\n",
      "\n",
      "\n",
      "['10887071', 'Show HN: Libconcurrent  Coroutines in C', 'https://github.com/sharow/libconcurrent', '91', '24', 'mitghi', '1/12/2016 12:50']\n",
      "\n",
      "\n",
      "['11596408', 'Show HN: Free Website Speed Test with Analytics and Improvement Tips', 'https://loadfocus.com', '1', '2', 'loadfocus', '4/29/2016 15:39']\n",
      "\n",
      "\n",
      "['12397346', 'Show HN: A Small SHMUP Game, Frequent Flyer', '', '2', '4', 'comrad_gremlin', '8/31/2016 10:55']\n",
      "\n",
      "\n",
      "Number of rows 1314\n",
      "Number of Columns 7\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('show posts Columns:',hn_header,'\\n')\n",
    "show_posts_5rows= explore_dataset(show_posts,0,5,True)\n",
    "print(show_posts_5rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other posts Columns: ['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'] \n",
      "\n",
      "['11579098', 'Rotonde: the IOT library', 'https://github.com/HackerLoop/rotonde/tree/master', '10', '2', 'ges', '4/27/2016 10:11']\n",
      "\n",
      "\n",
      "['10692656', 'Story of Diaspora community manager', 'https://medium.com/anti-fiction/planting-a-seed-what-working-at-diaspora-was-like-cde26fa29364#.gyo9c9aoy', '3', '1', 'zlatan_todoric', '12/7/2015 21:00']\n",
      "\n",
      "\n",
      "['10990621', 'Banking Model of Education', 'https://en.wikipedia.org/wiki/Banking_education', '3', '1', 'Kinnard', '1/28/2016 19:39']\n",
      "\n",
      "\n",
      "['12091931', 'The Downside to Cord-Cutting', 'http://www.nytimes.com/2016/07/14/technology/personaltech/the-downside-to-cord-cutting.html?smid=fb-nytimes&smtyp=cur&_r=0', '2', '1', 'prostoalex', '7/14/2016 5:59']\n",
      "\n",
      "\n",
      "['11425705', \"Millennials' new retirement number? $1.8M (or more)\", 'http://college.usatoday.com/2016/03/30/millennials-new-retirement-number-1-8-million-or-more/', '3', '2', 'JSeymourATL', '4/4/2016 21:12']\n",
      "\n",
      "\n",
      "Number of rows 17030\n",
      "Number of Columns 7\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('other posts Columns:',hn_header,'\\n')\n",
    "other_posts_5rows= explore_dataset(other_posts,0,5,True)\n",
    "print(other_posts_5rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Number of Comments\n",
    "We have to find now average number of comments for both ask_posts and show_posts. For this task we define a function ```avg_entity(dataset,n)``` where n is the index number of the entity for which average is to be determined in a given 'dataset'. It is assumed that the entity for which average is to be determined is listed as a string in the dataset and will, therefore, have to be converted to an integer or float.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_entity(dataset,n):\n",
    "    entity_list=[]\n",
    "    for row in dataset:\n",
    "        entity=row[n]\n",
    "        n_entity=int(entity)\n",
    "        entity_list.append(n_entity)\n",
    "    sum_entity=sum(entity_list)\n",
    "    length_entity=len(entity_list)\n",
    "    avg_entity=sum_entity/length_entity\n",
    "    return avg_entity    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We now use the function defined above to find average comments per post for both \"ask_posts' and \"show_posts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg ask comments 13.492149758454106 \n",
      " avg show comments 10.028158295281584\n"
     ]
    }
   ],
   "source": [
    "average_ask_comments=avg_entity(ask_posts,4)\n",
    "average_show_comments=avg_entity(show_posts,4)\n",
    "print('avg ask comments',average_ask_comments,'\\n', 'avg show comments',average_show_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding number of ask posts and number of comments by hour\n",
    "*** We now create a separate list named ```result_list``` to work with time and number of comments entities in ``ask_posts`` list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8/24/2016 20:27', 1], ['5/6/2016 15:37', 2], ['11/19/2015 15:42', 4], ['3/2/2016 20:08', 14], ['8/16/2016 20:33', 2]]\n",
      "1656\n"
     ]
    }
   ],
   "source": [
    "result_list=[]\n",
    "\n",
    "for row in ask_posts:\n",
    "    created_at=row[6]\n",
    "    num_comments=int(row[4])\n",
    "    data=[created_at,num_comments]\n",
    "    result_list.append(data)\n",
    "\n",
    "print(result_list[:5]) \n",
    "print(len(result_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We now use datetime module to calculate total posts and total comments by the hour for ask_posts***\n",
    "Note: We demonstrate different methods to extract date and time including 'formatting the string\", using datetime module, and strptime as well as strftime methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts by Hour {'20': 97, '15': 102, '23': 62, '14': 85, '17': 118, '21': 85, '13': 67, '11': 65, '22': 74, '07': 32, '18': 99, '19': 95, '03': 49, '12': 84, '06': 38, '09': 42, '10': 57, '02': 43, '01': 52, '16': 120, '00': 57, '08': 45, '05': 46, '04': 42} \n",
      "\n",
      "Comments by Hour {'20': 1211, '15': 2655, '23': 610, '14': 1014, '17': 2394, '21': 1192, '13': 1784, '11': 521, '22': 738, '07': 221, '18': 1116, '19': 778, '03': 332, '12': 1260, '06': 331, '09': 548, '10': 650, '02': 573, '01': 362, '16': 1628, '00': 967, '08': 597, '05': 403, '04': 458} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "counts_by_hour={}\n",
    "comments_by_hour={}\n",
    "for row in result_list:\n",
    "    created_at_str=row[0]\n",
    "    dt_time_str=created_at.split(' ')\n",
    "    date_str=dt_time_str[0]\n",
    "    time_str=dt_time_str[1]\n",
    "    time_str=time_str.split(':')\n",
    "    hour=int(time_str[0])\n",
    "    date=dt.datetime.strptime(created_at_str,'%m/%d/%Y %H:%M')\n",
    "    time_object=dt.datetime.time(date)\n",
    "    hour_object=time_object.hour\n",
    "    hour_str=dt.datetime.strftime(date,'%H')\n",
    "    if hour_str in counts_by_hour:\n",
    "        counts_by_hour[hour_str]+=1\n",
    "    else:\n",
    "        counts_by_hour[hour_str]=1\n",
    "    num_comments=row[1]    \n",
    "    if hour_str in comments_by_hour: \n",
    "        comments_by_hour[hour_str]+=num_comments\n",
    "    else:\n",
    "        comments_by_hour[hour_str]=num_comments\n",
    "    \n",
    "    \n",
    "print('Counts by Hour',counts_by_hour,'\\n',) \n",
    "print('Comments by Hour',comments_by_hour,'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Average Number of Ask Posts and Comments by Hour\n",
    "We now find average number of ask posts by the hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['20', 12.484536082474227], ['15', 26.029411764705884], ['23', 9.838709677419354], ['14', 11.929411764705883], ['17', 20.28813559322034], ['21', 14.023529411764706], ['13', 26.62686567164179], ['11', 8.015384615384615], ['22', 9.972972972972974], ['07', 6.90625], ['18', 11.272727272727273], ['19', 8.189473684210526], ['03', 6.775510204081633], ['12', 15.0], ['06', 8.710526315789474], ['09', 13.047619047619047], ['10', 11.403508771929825], ['02', 13.325581395348838], ['01', 6.961538461538462], ['16', 13.566666666666666], ['00', 16.964912280701753], ['08', 13.266666666666667], ['05', 8.76086956521739], ['04', 10.904761904761905]]\n"
     ]
    }
   ],
   "source": [
    "avg_by_hour=[]\n",
    "\n",
    "for hour_str in counts_by_hour:\n",
    "    avg_by_hour.append([hour_str,(comments_by_hour[hour_str]/counts_by_hour[hour_str])])\n",
    "    \n",
    "print(avg_by_hour)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Sorting and Printing Values\n",
    " We will now sort the above list in the descending order. \n",
    " First we swap the position of arguments on list ```avg_by_hour``` and store the values in a separate list ```swap_avg_by_hour```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.484536082474227, '20'], [26.029411764705884, '15'], [9.838709677419354, '23'], [11.929411764705883, '14'], [20.28813559322034, '17'], [14.023529411764706, '21'], [26.62686567164179, '13'], [8.015384615384615, '11'], [9.972972972972974, '22'], [6.90625, '07'], [11.272727272727273, '18'], [8.189473684210526, '19'], [6.775510204081633, '03'], [15.0, '12'], [8.710526315789474, '06'], [13.047619047619047, '09'], [11.403508771929825, '10'], [13.325581395348838, '02'], [6.961538461538462, '01'], [13.566666666666666, '16'], [16.964912280701753, '00'], [13.266666666666667, '08'], [8.76086956521739, '05'], [10.904761904761905, '04']]\n"
     ]
    }
   ],
   "source": [
    "swap_avg_by_hour=[]\n",
    "for row in avg_by_hour:\n",
    "    hour_str=row[0]\n",
    "    avg_comments=row[1]\n",
    "    swap_avg_by_hour.append([avg_comments,hour_str])\n",
    "print(swap_avg_by_hour)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting and Identifying Top Five Hours for Posts Comments\n",
    "Next we sort the list ```swap_avg_by_hour``` and store values in a list ``` sorted_swap```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26.62686567164179, '13'], [26.029411764705884, '15'], [20.28813559322034, '17'], [16.964912280701753, '00'], [15.0, '12']]\n",
      "\n",
      " Top 5 Hours for Ask Posts Comments\n",
      "13:00: 26.63 average comments per post\n",
      "15:00: 26.03 average comments per post\n",
      "17:00: 20.29 average comments per post\n",
      "00:00: 16.96 average comments per post\n",
      "12:00: 15.00 average comments per post\n"
     ]
    }
   ],
   "source": [
    "sorted_swap=sorted(swap_avg_by_hour,reverse=True)\n",
    "sorted_swap=sorted_swap[:5]\n",
    "print(sorted_swap[:5])\n",
    "print('\\n','Top 5 Hours for Ask Posts Comments')\n",
    "for row in sorted_swap:\n",
    "    avg=row[0]\n",
    "    hour=row[1]\n",
    "    hour_object=dt.datetime.strptime(hour,'%H')\n",
    "    hour_object_str=hour_object.strftime('%H:%M')\n",
    "    top_5=\"{time}: {avg:.2f} average comments per post\".format(time=hour_object_str,avg=avg)\n",
    "    print(top_5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Zones - Best Time to Post\n",
    "As per [documention](https://www.kaggle.com/hacker-news/hacker-news-posts#HN_posts_year_to_Sep_26_2016.csv) of the dataset, the time zone for created_at column is Eastern-US which is +9 hours ahead of our time zone. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
